{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_beautifulsoup_attempt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "U4-S1-NLP (Python3)",
      "language": "python",
      "name": "u4-s1-nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "nteract": {
      "version": "0.14.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bs3537/DS-Unit-4-Sprint-1-NLP/blob/master/Assignment_beautifulsoup_attempt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPs998shrTIf",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "\n",
        "# Vector Representations\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 2*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hyj-f9FDcVFp",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M7bcmqfGXrFG"
      },
      "source": [
        "## 1) *Optional:* Scrape 100 Job Listings that contain the title \"Data Scientist\" from indeed.com\n",
        "\n",
        "At a minimum your final dataframe of job listings should contain\n",
        "- Job Title\n",
        "- Job Description\n",
        "\n",
        "If you choose to not to scrape the data, there is a CSV with outdated data in the directory. Remeber, if you scrape Indeed, you're helping yourself find a job. ;)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5C4xFZNtX1m2"
      },
      "source": [
        "## 2) Use Spacy to tokenize / clean the listings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nbZH1gOrTIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjOcVzFtrTIw",
        "colab_type": "code",
        "colab": {},
        "outputId": "3a2275ac-4a92-40bb-fe41-fba74af26174"
      },
      "source": [
        "pip install beautifulsoup4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\bhavn\\anaconda3\\envs\\u4-s1-nlp\\lib\\site-packages (4.8.2)\n",
            "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\bhavn\\anaconda3\\envs\\u4-s1-nlp\\lib\\site-packages (from beautifulsoup4) (2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho3FPVylrTI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4XJr0A0rTI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://www.indeed.com/jobs?q=Data+Scientist&l=Boston%2C+MA\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSz4LHsIrTJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page = requests.get(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iHAcnvvrTJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "soup = BeautifulSoup(page.text, \"html.parser\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qYOp7clrTJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Information to be extracted from the above html page = job title, job description"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U10Qk7-7rTJV",
        "colab_type": "code",
        "colab": {},
        "outputId": "34574e63-80dc-4f78-c10a-443764cdad77"
      },
      "source": [
        "#Extracting job titles\n",
        "def extract_job_title_from_result(soup): \n",
        "    jobs = []\n",
        "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
        "        for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
        "            jobs.append(a[\"title\"])\n",
        "    return(jobs)\n",
        "title1 = extract_job_title_from_result(soup)\n",
        "title1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['OCR Data Scientist',\n",
              " 'Data Scientist',\n",
              " 'Data Scientist, Office of Data Science',\n",
              " 'Data Analyst/Scientist',\n",
              " 'Data Scientist',\n",
              " 'Data Scientist (Full-Time)',\n",
              " 'Data Scientist',\n",
              " 'Data Scientist',\n",
              " 'Juñior Data Scientist',\n",
              " 'Data Scientist/Economist',\n",
              " 'Data Scientist',\n",
              " 'Data Analyst/Scientist',\n",
              " 'Clinical Data Scientist',\n",
              " 'Data Scientist - Boston',\n",
              " 'Data Scientist',\n",
              " 'Assistant Director/Director, Office of Data Science']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaz6S99LrTJZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "8bbabab3-279b-4cbb-d526-2ebb6d968c1c"
      },
      "source": [
        "#Extracting listing companies\n",
        "def extract_company_from_result(soup): \n",
        "    companies = []\n",
        "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
        "        company = div.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
        "        if len(company) > 0:\n",
        "            for b in company:\n",
        "                companies.append(b.text.strip())\n",
        "        else:\n",
        "            sec_try = div.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
        "            for span in sec_try:\n",
        "                companies.append(span.text.strip())\n",
        "    return(companies)\n",
        " \n",
        "company1 = extract_company_from_result(soup)\n",
        "company1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['iQuartic',\n",
              " 'Fidelity Talent Source',\n",
              " 'Liberty Mutual Insurance',\n",
              " 'CGI Group, Inc.',\n",
              " 'SimpliSafe',\n",
              " 'proton.ai',\n",
              " 'Tekplay',\n",
              " 'GSK',\n",
              " 'PIÑATA SCIENCES',\n",
              " 'Burning Glass Technologies',\n",
              " 'Klaviyo',\n",
              " 'CGI Group, Inc.',\n",
              " 'ICHOM',\n",
              " 'Shift Technology',\n",
              " 'MORSE Corp',\n",
              " 'Liberty Mutual Insurance']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbvCArnArTJc",
        "colab_type": "code",
        "colab": {},
        "outputId": "6c3a555d-efa2-4992-8a71-c5377267ae58"
      },
      "source": [
        "#Extracting job descriptions\n",
        "def extract_summary_from_result(soup): \n",
        "    summaries = []\n",
        "    spans = soup.findAll('div', attrs={'class': 'summary'})\n",
        "    for span in spans:\n",
        "        summaries.append(span.text.strip())\n",
        "    return(summaries)\n",
        "summary1 = extract_summary_from_result(soup)\n",
        "summary1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The Senior Data Science Engineer will work closely with the software engineering team and domain experts at IQuartic, leverage OCR and machine learning…',\n",
              " 'Our associates are made up of a team of high caliber scientists, mathematicians and statisticians use rigorous quantitative approaches to ensure that we are…',\n",
              " 'Demonstrated experience in deep learning, computer vision, natural language processing, and/or interpretable machine learning.',\n",
              " 'For creating dashboards and delivering insights using visualization tools.\\nDevelop and implement analytics enabled solutions to improve business process,…',\n",
              " \"Skilled in SQL, R, Python, and statistical methods to analyze behavioral data.\\nYou'll be expected to creatively translate business problems into the appropriate…\",\n",
              " 'Salary: $75,000.00 to $150,000.00 /year.\\nWe will not review applications on indeed.',\n",
              " 'Experience with Data science tools R, Jupyter.\\nSalary: $80.00 to $90.00 /hour.\\nMust have minimum of 10 years of overall IT experience with at least 6 years of…',\n",
              " 'A higher degree in Engineering, Statistics, Data Science, Applied Mathematics, Computer Science, Physics, Computational Biology, Computational Chemistry or…',\n",
              " 'Experience with machine learning techniques.\\nPinata Scienes is looking for an up and coming Data Scientist.\\nThis data driven individual will work to support our…',\n",
              " 'Manipulating, validating, and analyzing data using SQL, R, Python, and other analytical tools to develop insights about emerging trends in the labor market.',\n",
              " 'The right candidate will have both a solid fundamental understanding and deep practical experience with at least a few modeling and machine learning techniques.',\n",
              " 'For creating dashboards and delivering insights using visualization tools.\\nDevelop and implement analytics enabled solutions to improve business process,…',\n",
              " 'Examples include Clinical Element Models, Archetype Definition Language (ADL), OpenEHR and clinical information modeling platforms such as Art Décor.',\n",
              " 'R&D for product development and innovation (through machine learning, image analysis, NLP, trend analysis, chatbots…).\\nTraining of junior Data scientists.',\n",
              " 'Solid understanding of physics, linear algebra, statistics, algorithms, optimization, and/or machine learning methods.',\n",
              " 'Competencies typically acquired through a Ph.D. degree (in Statistics, Mathematics, Economics, Actuarial Science or other scientific field of study) with 2+…']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-WRe4bXrTJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create empty dataframe to store scraped results\n",
        "columns = [\"title\", \"company_name\", \"summary\"]\n",
        "sample_df = pd.DataFrame(columns = columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95bbIZ2ErTJk",
        "colab_type": "code",
        "colab": {},
        "outputId": "a588feb1-ef77-49a1-8662-86fbf7747e37"
      },
      "source": [
        "sample_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>company_name</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [title, company_name, summary]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvvn8VwbrTJo",
        "colab_type": "code",
        "colab": {},
        "outputId": "a82548d5-b543-4b0f-b175-ea837718cad7"
      },
      "source": [
        "pip install lxml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lxml in c:\\users\\bhavn\\anaconda3\\envs\\u4-s1-nlp\\lib\\site-packages (4.5.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xojDY_BQrTJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_results_per_city = 100\n",
        "city_set = ['Boston']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2-QNwytrTJu",
        "colab_type": "code",
        "colab": {},
        "outputId": "afec22aa-7616-40cc-8ec5-1cb8fe3d3ad6"
      },
      "source": [
        "num = (len(sample_df) + 1)\n",
        "sample_df.loc[num] = summary1\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot set a row with mismatched columns",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-19-f101b03dd146>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_df\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msample_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msummary1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    443\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                                 raise ValueError(\"cannot set a row with \"\n\u001b[0m\u001b[0;32m    446\u001b[0m                                                  \"mismatched columns\")\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: cannot set a row with mismatched columns"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OZu4w1vrTJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nnnnnnnnnnnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVw_HHFmrTJ0",
        "colab_type": "code",
        "colab": {},
        "outputId": "3bdeca2d-21c2-4d56-9315-4b50f04758fa"
      },
      "source": [
        "#scraping code:\n",
        "for city in city_set:\n",
        "    for start in range(0, max_results_per_city, 100):\n",
        "        page = requests.get(\"https://www.indeed.com/jobs?q=Data+Scientist\" + str(city) + '&start=' + str(start))\n",
        "        time.sleep(1)  #ensuring at least 1 second between page grabs\n",
        "        soup = BeautifulSoup(page.text, \"html.parser\", from_encoding=\"utf-8\")\n",
        "    #soup = BeautifulSoup(page.text, \"html.parser\")\n",
        "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}): \n",
        "    #specifying row num for index of job posting in dataframe\n",
        "        num = (len(sample_df) + 1) \n",
        "    #creating an empty list to hold the data for each posting\n",
        "        job_post = [] \n",
        "    #grabbing job title\n",
        "    for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
        "        job_post.append(a[\"title\"])\n",
        "        \n",
        "    for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
        "        job_post.append(a[\"title\"])\n",
        "\n",
        "    #grabbing company name\n",
        "    #company = div.find_all(name=\"span\", attrs={\"class”:”company\"}) \n",
        "    #if len(company) > 0: \n",
        "        #for b in company:\n",
        "            #job_post.append(b.text.strip()) \n",
        "    #else: \n",
        "        #sec_try = div.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
        "        #for span in sec_try:\n",
        "            #job_post.append(span.text) \n",
        "    #grabbing summary text\n",
        "    d = div.findAll(\"div\", attrs={\"class\": \"summary\"}) \n",
        "    for div in d:\n",
        "        job_post.append(div.text.strip()) \n",
        "    #appending list of job post info to dataframe at index num\n",
        "    sample_df.loc[num] = job_post\n",
        "\n",
        "#saving sample_df as a local csv file — define your own local path to save contents \n",
        "#sample_df.to_csv(\"[filepath].csv\", encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot set a row with mismatched columns",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-82-475fa7565598>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mjob_post\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m#appending list of job post info to dataframe at index num\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0msample_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjob_post\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m#saving sample_df as a local csv file — define your own local path to save contents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    443\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                                 raise ValueError(\"cannot set a row with \"\n\u001b[0m\u001b[0;32m    446\u001b[0m                                                  \"mismatched columns\")\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: cannot set a row with mismatched columns"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGk_vbXarTJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-lgCZNL_YycP"
      },
      "source": [
        "## 3) Use Scikit-Learn's CountVectorizer to get word counts for each listing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0m24aPErTJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X2PZ8Pj_YxcF",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zo1iH_UeY7_n"
      },
      "source": [
        "## 4) Visualize the most common word counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M5LB00uyZKV5",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bwFsTqrVZMYi"
      },
      "source": [
        "## 5) Use Scikit-Learn's tfidfVectorizer to get a TF-IDF feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-gx2gZCbl5Np",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr5ZixhyrTKH",
        "colab_type": "text"
      },
      "source": [
        "## 6) Create a NearestNeighbor Model. Write the description of your ideal datascience job and query your job listings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "outputHidden": false,
        "id": "EqYZgv8grTKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FiDfTWceoRkH"
      },
      "source": [
        "## Stretch Goals\n",
        "\n",
        " - Try different visualizations for words and frequencies - what story do you want to tell with the data?\n",
        " - Scrape Job Listings for the job title \"Data Analyst\". How do these differ from Data Scientist Job Listings\n",
        " - Try and identify requirements for experience specific technologies that are asked for in the job listings. How are those distributed among the job listings?\n",
        " - Use a clustering algorithm to cluster documents by their most important terms. Do the clusters reveal any common themes?\n",
        "  - **Hint:** K-means might not be the best algorithm for this. Do a little bit of research to see what might be good for this. Also, remember that algorithms that depend on Euclidean distance break down with high dimensional data.\n",
        " - Create a labeled dataset - which jobs will you apply for? Train a model to select the jobs you are most likely to apply for. :) "
      ]
    }
  ]
}